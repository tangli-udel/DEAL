{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "from load import *\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "from explainer import gradCAM, interpret\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from scipy.ndimage import filters\n",
    "from loss import BatchSeparationLoss, BatchConsistencyLoss, SparsityLoss\n",
    "\n",
    "seed_everything(hparams['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, seed, bs):\n",
    "    # Create a RandomSampler with a fixed seed for shuffling\n",
    "    sampler = RandomSampler(dataset, replacement=False, num_samples=None, generator=torch.Generator().manual_seed(seed))\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=bs,\n",
    "        sampler=sampler,\n",
    "        num_workers=16, \n",
    "        pin_memory=False\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageNet(hparams['data_dir'], split='val', transform=tfms)\n",
    "# dataset = CUBDataset(hparams['data_dir'], train=False, transform=tfms)\n",
    "# dataset = torchvision.datasets.OxfordIIITPet(root=hparams['data_dir'], transform=tfms, split='test')\n",
    "# dataset = test_set # EuroSAT\n",
    "# dataset = torchvision.datasets.Food101(root=hparams['data_dir'], transform=tfms, split='test')\n",
    "\n",
    "bs = 8\n",
    "seed = 123\n",
    "dataloader = get_dataloader(dataset, seed, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(hparams['device'])\n",
    "model, preprocess = clip.load(hparams['model_size'], device=device, jit=False) #Best model use ViT-B/32\n",
    "checkpoint = torch.load(\"/path/to/your/model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_to_metric(loss, k=50, threshold=23):\n",
    "    # Ensure the loss is a tensor\n",
    "    if not isinstance(loss, torch.Tensor):\n",
    "        loss = torch.tensor(loss)\n",
    "    \n",
    "    return (1 - torch.sigmoid(-k * (loss - threshold))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(loss):\n",
    "    return (1 - (loss / 100)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sep = BatchSeparationLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_disentanglability = 0.0\n",
    "count = 0 \n",
    "for batch_number, batch in enumerate(tqdm(dataloader)):\n",
    "    images, labels = batch\n",
    "    texts = np.array(label_to_classname)[labels].tolist()\n",
    "\n",
    "    tokenized_concepts_list = []\n",
    "    rich_labels = []\n",
    "    for i in range(len(texts)):\n",
    "        concepts = gpt_descriptions[texts[i]][:5]\n",
    "        concatenated_concepts = ', '.join(concepts)\n",
    "        label = hparams['label_before_text'] + wordify(texts[i]) + hparams['label_after_text'] + \" It may contains \" + concatenated_concepts\n",
    "        rich_labels.append(label)\n",
    "        \n",
    "        concepts.insert(0, texts[i])\n",
    "        tokenized_concepts = clip.tokenize(concepts)\n",
    "        tokenized_concepts_list.append(tokenized_concepts)\n",
    "\n",
    "    images = images.to(device)\n",
    "    texts = clip.tokenize(texts)\n",
    "    texts = texts.to(device)\n",
    "\n",
    "    attn_map = []\n",
    "    if hparams['model_size'] in ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64']:\n",
    "        for k in range(len(images)):\n",
    "            num_texts = tokenized_concepts_list[k].shape[0]\n",
    "            repeated_image = images[k].unsqueeze(0).repeat(num_texts, 1, 1, 1)\n",
    "            heatmap = gradCAM(\n",
    "                model.visual,\n",
    "                repeated_image,\n",
    "                model.encode_text(tokenized_concepts_list[k].to(device)),\n",
    "                getattr(model.visual, \"layer4\")\n",
    "            )\n",
    "            attn_map.append(heatmap)\n",
    "    \n",
    "    elif hparams['model_size'] in ['ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']:\n",
    "        for k in range(len(images)):\n",
    "            R_image = interpret(model=model, image=images[k].unsqueeze(0), texts=tokenized_concepts_list[k].to(device), device=device)\n",
    "            image_relevance = R_image[0]\n",
    "            dim = int(image_relevance.numel() ** 0.5)\n",
    "            R_image = R_image.reshape(-1, dim, dim)\n",
    "            attn_map.append(R_image)\n",
    "    \n",
    "    attn_map_label = [item[0] for item in attn_map]\n",
    "    attn_map_concepts = [item[1:] for item in attn_map]\n",
    "\n",
    "    total_disentanglability += loss_sep(attn_map_concepts) / 100\n",
    "    count += 1\n",
    "\n",
    "avg_disen = total_disentanglability / count\n",
    "print ('The final explanation disentanglability: ' + str(avg_disen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_metric(loss, k=10, M=0.2, temp=1.0):\n",
    "    \"\"\"\n",
    "    Transform a loss value into a metric using a sigmoid function.\n",
    "    \n",
    "    Returns:\n",
    "    - A metric value between 0 and 1, where lower losses yield higher metrics.\n",
    "    \"\"\"\n",
    "    # Apply the sigmoid transformation\n",
    "    metric = 1 / (1 + np.exp(-k * (M - loss / temp)))\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final metric value\n",
    "sigmoid_metric(avg_disen.detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
